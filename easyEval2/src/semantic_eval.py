#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯­ä¹‰è¯„ä¼°å¼•æ“Žæ¨¡å—
è´Ÿè´£æ ¸å¿ƒçš„è¯­ä¹‰ç›¸ä¼¼åº¦è¯„ä¼°é€»è¾‘
"""

import json
import time
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict

from config.config import config
from src.deepseek_client import DeepSeekClient
from src.local_api_client import LocalAPIClient

@dataclass
class TestCase:
    """æµ‹è¯•ç”¨ä¾‹æ•°æ®ç±»"""
    id: str
    question: str
    category: str = "general"
    expected_aspects: List[str] = None
    priority: str = "medium"
    scenario: str = "general"
    
    def __post_init__(self):
        if self.expected_aspects is None:
            self.expected_aspects = []

@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æžœæ•°æ®ç±»"""
    test_id: str
    question: str
    answer: str
    semantic_score: int
    evaluation_reason: str
    dimension_scores: Dict[str, int]
    scenario: str
    timestamp: str
    api_response_time: float = 0.0
    raw_response: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)

class SemanticEvaluator:
    """è¯­ä¹‰è¯„ä¼°å™¨"""
    
    def __init__(self, use_local_api: bool = False, local_api_url: str = "http://localhost:8000"):
        """åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            use_local_api: æ˜¯å¦ä½¿ç”¨æœ¬åœ°APIå®¢æˆ·ç«¯
            local_api_url: æœ¬åœ°APIæœåŠ¡å™¨åœ°å€
        """
        self.logger = logging.getLogger(__name__)
        
        # æ ¹æ®å‚æ•°é€‰æ‹©å®¢æˆ·ç«¯
        if use_local_api:
            self.api_client = LocalAPIClient(local_api_url)
            self.logger.info(f"ä½¿ç”¨æœ¬åœ°APIå®¢æˆ·ç«¯: {local_api_url}")
        else:
            self.api_client = DeepSeekClient()
            self.logger.info("ä½¿ç”¨DeepSeek APIå®¢æˆ·ç«¯")
            
        # ä¿æŒå‘åŽå…¼å®¹æ€§
        self.deepseek_client = self.api_client if not use_local_api else None
        
        self.results: List[EvaluationResult] = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_tests': 0,
            'completed_tests': 0,
            'failed_tests': 0,
            'average_score': 0.0,
            'total_api_time': 0.0,
            'start_time': None,
            'end_time': None
        }
        
        self.logger.info("è¯­ä¹‰è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_test_cases(self, test_file: str) -> List[TestCase]:
        """åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
        
        test_path = Path(test_file)
        if not test_path.exists():
            raise FileNotFoundError(f"æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        
        try:
            with open(test_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            test_cases = []
            
            # æ”¯æŒä¸¤ç§æ ¼å¼ï¼šç›´æŽ¥åˆ—è¡¨æˆ–åŒ…å«test_caseså­—æ®µçš„å¯¹è±¡
            if isinstance(data, list):
                cases_data = data
            elif isinstance(data, dict) and 'test_cases' in data:
                cases_data = data['test_cases']
            else:
                raise ValueError("æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
            
            for case_data in cases_data:
                test_case = TestCase(
                    id=case_data.get('id', f"test_{len(test_cases)+1}"),
                    question=case_data['question'],
                    category=case_data.get('category', 'general'),
                    expected_aspects=case_data.get('expected_aspects', []),
                    priority=case_data.get('priority', 'medium'),
                    scenario=case_data.get('scenario', 'general')
                )
                test_cases.append(test_case)
            
            self.logger.info(f"æˆåŠŸåŠ è½½ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
            return test_cases
            
        except Exception as e:
            self.logger.error(f"åŠ è½½æµ‹è¯•ç”¨ä¾‹å¤±è´¥: {str(e)}")
            raise
    
    def get_easychat_response(self, question: str) -> Optional[str]:
        """èŽ·å–EasyChatçš„å›žç­”"""
        
        # è¿™é‡Œåº”è¯¥è°ƒç”¨EasyChat API
        # ç›®å‰ä½¿ç”¨æ¨¡æ‹Ÿå›žç­”è¿›è¡Œæµ‹è¯•
        
        import requests
        
        try:
            self.logger.debug(f"å‘EasyChatå‘é€é—®é¢˜: {question[:50]}...")
            
            # æž„å»ºè¯·æ±‚
            url = f"{config.easychat.url}/chat"
            payload = {
                "message": question,
                "session_id": "eval_session"
            }
            
            response = requests.post(
                url, 
                json=payload, 
                timeout=config.easychat.timeout
            )
            
            if response.status_code == 200:
                data = response.json()
                answer = data.get('response', data.get('message', ''))
                self.logger.debug(f"EasyChatå›žç­”: {answer[:50]}...")
                return answer
            else:
                self.logger.warning(f"EasyChat APIè¿”å›žé”™è¯¯çŠ¶æ€: {response.status_code}")
                return None
                
        except requests.exceptions.RequestException as e:
            self.logger.warning(f"EasyChat APIè°ƒç”¨å¤±è´¥: {str(e)}")
            # è¿”å›žæ¨¡æ‹Ÿå›žç­”ç”¨äºŽæµ‹è¯•
            return self._get_mock_answer(question)
        except Exception as e:
            self.logger.error(f"èŽ·å–EasyChatå›žç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None
    
    def _get_mock_answer(self, question: str) -> str:
        """èŽ·å–æ¨¡æ‹Ÿå›žç­”ï¼ˆç”¨äºŽæµ‹è¯•ï¼‰"""
        
        mock_answers = {
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºŽåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
            "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹": "å­¦ä¹ ç¼–ç¨‹å¯ä»¥ä»Žé€‰æ‹©ä¸€é—¨ç¼–ç¨‹è¯­è¨€å¼€å§‹ï¼Œæ¯”å¦‚Pythonï¼Œç„¶åŽé€šè¿‡åœ¨çº¿æ•™ç¨‹ã€ä¹¦ç±å’Œå®žè·µé¡¹ç›®æ¥é€æ­¥æé«˜æŠ€èƒ½ã€‚",
            "æŽ¨èä¸€éƒ¨ç”µå½±": "æˆ‘æŽ¨èã€Šè‚–ç”³å…‹çš„æ•‘èµŽã€‹ï¼Œè¿™æ˜¯ä¸€éƒ¨å…³äºŽå¸Œæœ›å’Œå‹è°Šçš„ç»å…¸ç”µå½±ï¼Œæƒ…èŠ‚æ„Ÿäººï¼Œæ¼”æŠ€å‡ºè‰²ã€‚",
            "ä»Šå¤©å¤©æ°”æ€Žä¹ˆæ ·": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•èŽ·å–å®žæ—¶å¤©æ°”ä¿¡æ¯ã€‚å»ºè®®æ‚¨æŸ¥çœ‹å¤©æ°”é¢„æŠ¥åº”ç”¨æˆ–ç½‘ç«™èŽ·å–å‡†ç¡®çš„å¤©æ°”ä¿¡æ¯ã€‚"
        }
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        for key, answer in mock_answers.items():
            if key in question:
                return answer
        
        return "è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ï¼Œæˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚ä¸è¿‡æˆ‘éœ€è¦æ›´å¤šä¿¡æ¯æ‰èƒ½ç»™å‡ºå‡†ç¡®çš„å›žç­”ã€‚"
    
    def evaluate_single(self, test_case: TestCase) -> Optional[EvaluationResult]:
        """è¯„ä¼°å•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        
        try:
            self.logger.info(f"å¼€å§‹è¯„ä¼°æµ‹è¯•ç”¨ä¾‹: {test_case.id}")
            
            # èŽ·å–AIå›žç­”
            start_time = time.time()
            answer = self.get_easychat_response(test_case.question)
            
            if not answer:
                self.logger.error(f"æ— æ³•èŽ·å–æµ‹è¯•ç”¨ä¾‹ {test_case.id} çš„å›žç­”")
                return None
            
            # è¿›è¡Œè¯­ä¹‰è¯„ä¼°
            api_start_time = time.time()
            evaluation = self.api_client.evaluate_semantic_similarity(
                test_case.question, 
                answer, 
                test_case.scenario
            )
            api_response_time = time.time() - api_start_time
            
            if not evaluation:
                self.logger.error(f"æµ‹è¯•ç”¨ä¾‹ {test_case.id} çš„è¯­ä¹‰è¯„ä¼°å¤±è´¥")
                return None
            
            # æž„å»ºè¯„ä¼°ç»“æžœ
            result = EvaluationResult(
                test_id=test_case.id,
                question=test_case.question,
                answer=answer,
                semantic_score=evaluation['score'],
                evaluation_reason=evaluation['reason'],
                dimension_scores=evaluation.get('dimensions', {}),
                scenario=test_case.scenario,
                timestamp=datetime.now().isoformat(),
                api_response_time=api_response_time,
                raw_response=evaluation.get('raw_response')
            )
            
            self.logger.info(f"æµ‹è¯•ç”¨ä¾‹ {test_case.id} è¯„ä¼°å®Œæˆï¼Œå¾—åˆ†: {result.semantic_score}")
            return result
            
        except Exception as e:
            self.logger.error(f"è¯„ä¼°æµ‹è¯•ç”¨ä¾‹ {test_case.id} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None
    
    def evaluate_batch(self, test_cases: List[TestCase], 
                      progress_callback=None) -> List[EvaluationResult]:
        """æ‰¹é‡è¯„ä¼°æµ‹è¯•ç”¨ä¾‹"""
        
        self.logger.info(f"å¼€å§‹æ‰¹é‡è¯„ä¼° {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        self.stats['total_tests'] = len(test_cases)
        self.stats['start_time'] = datetime.now().isoformat()
        
        results = []
        
        for i, test_case in enumerate(test_cases):
            try:
                # è¯„ä¼°å•ä¸ªç”¨ä¾‹
                result = self.evaluate_single(test_case)
                
                if result:
                    results.append(result)
                    self.stats['completed_tests'] += 1
                    self.stats['total_api_time'] += result.api_response_time
                else:
                    self.stats['failed_tests'] += 1
                
                # è¿›åº¦å›žè°ƒ - åœ¨è¯„ä¼°å®ŒæˆåŽè°ƒç”¨
                if progress_callback:
                    progress_callback(i, len(test_cases), test_case.id)
                
                # æ˜¾ç¤ºè¿›åº¦ï¼ˆä»…åœ¨æ²¡æœ‰è¿›åº¦å›žè°ƒæ—¶æ˜¾ç¤ºï¼‰
                if not progress_callback:
                    progress = (i + 1) / len(test_cases) * 100
                    self.logger.info(f"è¿›åº¦: {progress:.1f}% ({i+1}/{len(test_cases)})")
                
            except KeyboardInterrupt:
                self.logger.warning("ç”¨æˆ·ä¸­æ–­è¯„ä¼°è¿‡ç¨‹")
                break
            except Exception as e:
                self.logger.error(f"å¤„ç†æµ‹è¯•ç”¨ä¾‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                self.stats['failed_tests'] += 1
                # å³ä½¿å‡ºé”™ä¹Ÿè¦æ›´æ–°è¿›åº¦
                if progress_callback:
                    progress_callback(i, len(test_cases), test_case.id)
                continue
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.stats['end_time'] = datetime.now().isoformat()
        if results:
            self.stats['average_score'] = sum(r.semantic_score for r in results) / len(results)
        
        self.results = results
        self.logger.info(f"æ‰¹é‡è¯„ä¼°å®Œæˆï¼ŒæˆåŠŸ: {len(results)}, å¤±è´¥: {self.stats['failed_tests']}")
        
        return results
    
    def save_results(self, output_file: str) -> bool:
        """ä¿å­˜è¯„ä¼°ç»“æžœ"""
        
        try:
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # æž„å»ºå®Œæ•´çš„æŠ¥å‘Šæ•°æ®
            report_data = {
                'metadata': {
                    'evaluation_time': datetime.now().isoformat(),
                    'evaluator_version': '2.0.0',
                    'total_tests': len(self.results),
                    'statistics': self.stats
                },
                'results': [result.to_dict() for result in self.results],
                'summary': self._generate_summary()
            }
            
            # ä¿å­˜JSONæŠ¥å‘Š
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            # ç”Ÿæˆå¹¶ä¿å­˜MarkdownæŠ¥å‘Š
            md_output_file = str(output_path).replace('.json', '.md')
            self.save_markdown_summary(md_output_file)
            
            self.logger.info(f"è¯„ä¼°ç»“æžœå·²ä¿å­˜åˆ°: {output_file}")
            self.logger.info(f"Markdownæ‘˜è¦å·²ä¿å­˜åˆ°: {md_output_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜è¯„ä¼°ç»“æžœå¤±è´¥: {str(e)}")
            return False
    
    def save_markdown_summary(self, output_file: str) -> bool:
        """ä¿å­˜Markdownæ ¼å¼çš„è¯„ä¼°æ‘˜è¦æŠ¥å‘Š"""
        
        try:
            if not self.results:
                return False
            
            summary = self._generate_summary()
            md_content = self._generate_markdown_report(summary)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(md_content)
            
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜Markdownæ‘˜è¦å¤±è´¥: {str(e)}")
            return False
    
    def _generate_markdown_report(self, summary: Dict[str, Any]) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„è¯„ä¼°æŠ¥å‘Š"""
        
        md_lines = []
        
        # æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯
        md_lines.append("# è¯­ä¹‰è¯„ä¼°ç»“æžœæ‘˜è¦")
        md_lines.append("")
        md_lines.append(f"**è¯„ä¼°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        md_lines.append(f"**è¯„ä¼°å™¨ç‰ˆæœ¬**: 2.0.0")
        md_lines.append("")
        
        # æ€»ä½“ç»Ÿè®¡
        md_lines.append("## ðŸ“Š æ€»ä½“ç»Ÿè®¡")
        md_lines.append("")
        md_lines.append(f"- **æ€»æµ‹è¯•æ•°**: {summary['total_tests']}")
        md_lines.append(f"- **å¹³å‡åˆ†æ•°**: {summary['average_score']:.1f}")
        md_lines.append(f"- **æœ€é«˜åˆ†æ•°**: {summary['max_score']}")
        md_lines.append(f"- **æœ€ä½Žåˆ†æ•°**: {summary['min_score']}")
        md_lines.append("")
        
        # åˆ†æ•°åˆ†å¸ƒ
        md_lines.append("## ðŸ“ˆ åˆ†æ•°åˆ†å¸ƒ")
        md_lines.append("")
        dist = summary['score_distribution']
        md_lines.append("| ç­‰çº§ | åˆ†æ•°èŒƒå›´ | æ•°é‡ | ç™¾åˆ†æ¯” |")
        md_lines.append("|------|----------|------|--------|")
        total = summary['total_tests']
        md_lines.append(f"| ðŸŒŸ ä¼˜ç§€ | 90-100 | {dist['excellent']} | {dist['excellent']/total*100:.1f}% |")
        md_lines.append(f"| ðŸ‘ è‰¯å¥½ | 80-89 | {dist['good']} | {dist['good']/total*100:.1f}% |")
        md_lines.append(f"| ðŸ˜ ä¸€èˆ¬ | 70-79 | {dist['average']} | {dist['average']/total*100:.1f}% |")
        md_lines.append(f"| ðŸ˜• è¾ƒå·® | 60-69 | {dist['poor']} | {dist['poor']/total*100:.1f}% |")
        md_lines.append(f"| ðŸ˜ž å¾ˆå·® | 0-59 | {dist['very_poor']} | {dist['very_poor']/total*100:.1f}% |")
        md_lines.append("")
        
        # åœºæ™¯ç»Ÿè®¡
        md_lines.append("## ðŸŽ¯ åœºæ™¯ç»Ÿè®¡")
        md_lines.append("")
        md_lines.append("| åœºæ™¯ | æµ‹è¯•æ•°é‡ | å¹³å‡åˆ†æ•° |")
        md_lines.append("|------|----------|----------|")
        for scenario, stats in summary['scenario_statistics'].items():
            md_lines.append(f"| {scenario} | {stats['count']} | {stats['average_score']:.1f} |")
        md_lines.append("")
        
        # æ€§èƒ½æŒ‡æ ‡
        md_lines.append("## âš¡ æ€§èƒ½æŒ‡æ ‡")
        md_lines.append("")
        perf = summary['performance_metrics']
        md_lines.append(f"- **æˆåŠŸçŽ‡**: {perf['success_rate']:.1f}%")
        md_lines.append(f"- **æ€»APIæ—¶é—´**: {perf['total_api_time']:.2f} ç§’")
        md_lines.append(f"- **å¹³å‡APIæ—¶é—´**: {perf['average_api_time']:.2f} ç§’")
        md_lines.append("")
        
        # è¯¦ç»†ç»“æžœï¼ˆä»…æ˜¾ç¤ºå‰10ä¸ªï¼‰
        md_lines.append("## ðŸ“‹ è¯¦ç»†ç»“æžœ (å‰10ä¸ª)")
        md_lines.append("")
        md_lines.append("| æµ‹è¯•ID | åœºæ™¯ | åˆ†æ•° | è¯„ä¼°ç†ç”± |")
        md_lines.append("|--------|------|------|----------|")
        
        for i, result in enumerate(self.results[:10]):
            reason_short = result.evaluation_reason[:50] + "..." if len(result.evaluation_reason) > 50 else result.evaluation_reason
            md_lines.append(f"| {result.test_id} | {result.scenario} | {result.semantic_score} | {reason_short} |")
        
        if len(self.results) > 10:
            md_lines.append(f"| ... | ... | ... | è¿˜æœ‰ {len(self.results) - 10} ä¸ªç»“æžœ |")
        
        md_lines.append("")
        
        # ç”Ÿæˆæ—¶é—´æˆ³
        md_lines.append("---")
        md_lines.append(f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*")
        
        return "\n".join(md_lines)
    
    def _generate_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆè¯„ä¼°æ‘˜è¦"""
        
        if not self.results:
            return {}
        
        scores = [r.semantic_score for r in self.results]
        
        # åˆ†æ•°åˆ†å¸ƒ
        score_distribution = {
            'excellent': len([s for s in scores if s >= 90]),
            'good': len([s for s in scores if 80 <= s < 90]),
            'average': len([s for s in scores if 70 <= s < 80]),
            'poor': len([s for s in scores if 60 <= s < 70]),
            'very_poor': len([s for s in scores if s < 60])
        }
        
        # åœºæ™¯ç»Ÿè®¡
        scenario_stats = {}
        for result in self.results:
            scenario = result.scenario
            if scenario not in scenario_stats:
                scenario_stats[scenario] = {'count': 0, 'total_score': 0}
            scenario_stats[scenario]['count'] += 1
            scenario_stats[scenario]['total_score'] += result.semantic_score
        
        # è®¡ç®—åœºæ™¯å¹³å‡åˆ†
        for scenario, stats in scenario_stats.items():
            stats['average_score'] = stats['total_score'] / stats['count']
        
        return {
            'total_tests': len(self.results),
            'average_score': sum(scores) / len(scores),
            'max_score': max(scores),
            'min_score': min(scores),
            'score_distribution': score_distribution,
            'scenario_statistics': scenario_stats,
            'performance_metrics': {
                'total_api_time': self.stats['total_api_time'],
                'average_api_time': self.stats['total_api_time'] / len(self.results),
                'success_rate': len(self.results) / self.stats['total_tests'] * 100
            }
        }
    
    def print_summary(self):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        
        if not self.results:
            print("æ²¡æœ‰è¯„ä¼°ç»“æžœ")
            return
        
        summary = self._generate_summary()
        
        print("\n" + "="*50)
        print("è¯­ä¹‰è¯„ä¼°ç»“æžœæ‘˜è¦")
        print("="*50)
        
        print(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"å¹³å‡åˆ†æ•°: {summary['average_score']:.1f}")
        print(f"æœ€é«˜åˆ†æ•°: {summary['max_score']}")
        print(f"æœ€ä½Žåˆ†æ•°: {summary['min_score']}")
        
        print("\nåˆ†æ•°åˆ†å¸ƒ:")
        dist = summary['score_distribution']
        print(f"  ä¼˜ç§€ (90-100): {dist['excellent']} ä¸ª")
        print(f"  è‰¯å¥½ (80-89):  {dist['good']} ä¸ª")
        print(f"  ä¸€èˆ¬ (70-79):  {dist['average']} ä¸ª")
        print(f"  è¾ƒå·® (60-69):  {dist['poor']} ä¸ª")
        print(f"  å¾ˆå·® (0-59):   {dist['very_poor']} ä¸ª")
        
        print("\nåœºæ™¯ç»Ÿè®¡:")
        for scenario, stats in summary['scenario_statistics'].items():
            print(f"  {scenario}: {stats['count']} ä¸ªæµ‹è¯•ï¼Œå¹³å‡åˆ† {stats['average_score']:.1f}")
        
        print("\næ€§èƒ½æŒ‡æ ‡:")
        perf = summary['performance_metrics']
        print(f"  æˆåŠŸçŽ‡: {perf['success_rate']:.1f}%")
        print(f"  æ€»APIæ—¶é—´: {perf['total_api_time']:.2f} ç§’")
        print(f"  å¹³å‡APIæ—¶é—´: {perf['average_api_time']:.2f} ç§’")
        
        print("="*50)

def main():
    """ä¸»å‡½æ•°"""
    
    import argparse
    import sys
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=getattr(logging, config.log.level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(config.log.file),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    # è§£æžå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='è¯­ä¹‰ç›¸ä¼¼åº¦è¯„ä¼°å·¥å…·')
    parser.add_argument('--test-file', '-t', 
                       default='tests/test_cases.json',
                       help='æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o',
                       default=None,
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--verbose', '-v',
                       action='store_true',
                       help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = SemanticEvaluator()
        
        # æµ‹è¯•APIè¿žæŽ¥
        print("æµ‹è¯•DeepSeek APIè¿žæŽ¥...")
        if not evaluator.deepseek_client.test_connection():
            print("âŒ APIè¿žæŽ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
            return 1
        print("âœ… APIè¿žæŽ¥æ­£å¸¸")
        
        # åŠ è½½æµ‹è¯•ç”¨ä¾‹
        print(f"\nåŠ è½½æµ‹è¯•ç”¨ä¾‹: {args.test_file}")
        test_cases = evaluator.load_test_cases(args.test_file)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        
        # è¿›åº¦å›žè°ƒå‡½æ•°
        def progress_callback(current, total, test_id):
            progress = (current + 1) / total * 100
            print(f"\rè¿›åº¦: {progress:.1f}% - æ­£åœ¨è¯„ä¼°: {test_id}", end='', flush=True)
        
        # å¼€å§‹è¯„ä¼°
        print("\nå¼€å§‹è¯­ä¹‰è¯„ä¼°...")
        results = evaluator.evaluate_batch(test_cases, progress_callback)
        print()  # æ¢è¡Œ
        
        # æ˜¾ç¤ºæ‘˜è¦
        evaluator.print_summary()
        
        # ä¿å­˜ç»“æžœ
        if args.output:
            output_file = args.output
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"results/semantic_eval_{timestamp}.json"
        
        if evaluator.save_results(output_file):
            print(f"\nâœ… è¯„ä¼°ç»“æžœå·²ä¿å­˜åˆ°: {output_file}")
        else:
            print("\nâŒ ä¿å­˜è¯„ä¼°ç»“æžœå¤±è´¥")
            return 1
        
        return 0
        
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­è¯„ä¼°")
        return 1
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}")
        return 1

if __name__ == '__main__':
    exit(main())